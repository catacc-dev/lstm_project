{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, LSTM, Dropout, Dense, Softmax\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SmilesToOneHotEncoding Class\n",
        "The main class that handles the complete pipeline for converting SMILES strings to one-hot encoded sequences suitable for LSTM training. It includes methods for data loading, preprocessing, tokenization, encoding, model building, training, and molecule generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybNbRF9hB018"
      },
      "outputs": [],
      "source": [
        "class SmilesToOneHotEncoding:\n",
        "    \"\"\"\n",
        "    A class to convert SMILES (Simplified Molecular Input Line Entry System) strings\n",
        "    to one-hot encoded sequences, suitable for training machine learning models like LSTMs.\n",
        "    It handles data loading, translation of specific characters, padding, vocabulary creation,\n",
        "    and one-hot encoding of SMILES strings.\n",
        "    \"\"\"\n",
        "    def __init__(self, filename, size_dataset = 100,  min_length = 20, max_length = 100):\n",
        "        \"\"\"\n",
        "        Initializes the SmilesToOneHotEncoding class with dataset parameters and\n",
        "        performs initial data preprocessing steps.\n",
        "\n",
        "        Args:\n",
        "            filename (str): Path to the file containing SMILES strings.\n",
        "            size_dataset (int): The number of SMILES strings to load from the dataset.\n",
        "                                Defaults to 100.\n",
        "            min_length (int): Minimum length of SMILES strings to be considered.\n",
        "                              Molecules shorter than this will be discarded.\n",
        "                              Defaults to 20.\n",
        "            max_length (int): Maximum length to pad/truncate SMILES strings to.\n",
        "                              Molecules longer than this will be discarded.\n",
        "                              Defaults to 100.\n",
        "        \"\"\"\n",
        "        # Constructor initializes parameters and preprocesses data\n",
        "        self.filename = filename\n",
        "        self.size_dataset = size_dataset\n",
        "        self.min_length = min_length\n",
        "        self.max_length = max_length\n",
        "        # Preprocess data upon initialization: load, translate, and pad molecules\n",
        "        self.molecule_list = self.preprocess_data()\n",
        "        # Create tokenizer and character-to-index mapping for encoding\n",
        "        self.tokenizer, self.char_to_index = self.make_vocabulary()\n",
        "\n",
        "    def load_file(self):\n",
        "        \"\"\"\n",
        "        Loads SMILES strings from the specified file.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of SMILES strings, with each line truncated at the first comma.\n",
        "        \"\"\"\n",
        "        # load file from datasets\n",
        "        with open(self.filename, 'r') as file:\n",
        "            filecontent = file.readlines()\n",
        "\n",
        "        # split each line using comma as the delimiter and take the first part\n",
        "        for i in range(len(filecontent)):\n",
        "            filecontent[i] = filecontent[i].split(\" ,\")[0]\n",
        "        return filecontent\n",
        "\n",
        "    def translation(self, molecule_list):\n",
        "        \"\"\"\n",
        "        Translates specific characters in the SMILES strings to simplify the vocabulary.\n",
        "        Specifically, 'Br' is replaced with 'R' and 'Cl' with 'L'.\n",
        "\n",
        "        Args:\n",
        "            molecule_list (list): A list of raw SMILES strings.\n",
        "\n",
        "        Returns:\n",
        "            list: The list of SMILES strings with specified characters translated.\n",
        "        \"\"\"\n",
        "        for i in range(len(molecule_list)):\n",
        "            molecule_list[i] = molecule_list[i].replace('Br','R') # Replace Bromine with 'R'\n",
        "            molecule_list[i] = molecule_list[i].replace('Cl','L') # Replace Chlorine with 'L'\n",
        "        return molecule_list\n",
        "\n",
        "    def padding(self, molecule_list):\n",
        "        \"\"\"\n",
        "        Pads SMILES strings with special tokens ('G' for start, 'E' for end, 'A' for padding)\n",
        "        to a uniform `max_length`. Molecules outside `min_length` and `max_length` bounds are filtered.\n",
        "\n",
        "        Args:\n",
        "            molecule_list (list): A list of translated SMILES strings.\n",
        "\n",
        "        Returns:\n",
        "            list: A new list of padded and filtered SMILES strings.\n",
        "        \"\"\"\n",
        "        new_list = []\n",
        "        for i in range(len(molecule_list)):\n",
        "            # Only consider molecules within the specified length range\n",
        "            if not len(molecule_list[i]) >= self.max_length and not len(molecule_list[i]) < self.min_length:\n",
        "                molecule = molecule_list[i]\n",
        "                # Add start 'G' and end 'E' tokens\n",
        "                molecule = 'G' + molecule + 'E'\n",
        "                # Pad with 'A' characters to reach max_length\n",
        "                molecule =  molecule + 'A' * (self.max_length - len(molecule))\n",
        "                new_list.append(molecule)\n",
        "        return new_list\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"\n",
        "        Orchestrates the data preprocessing pipeline: loading, character translation,\n",
        "        and padding/filtering of SMILES strings.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of fully preprocessed SMILES strings.\n",
        "        \"\"\"\n",
        "        # Load a subset of the file based on size_dataset\n",
        "        molecule_list = self.load_file()[0:self.size_dataset]\n",
        "        # Translate specific characters in the molecule list\n",
        "        molecule_list = self.translation(molecule_list)\n",
        "        # Pad molecules and filter by length\n",
        "        molecule_list = self.padding(molecule_list)\n",
        "        return molecule_list\n",
        "\n",
        "    def make_vocabulary(self):\n",
        "        \"\"\"\n",
        "        Creates a character-level vocabulary from the preprocessed SMILES strings\n",
        "        using Keras's Tokenizer, and generates a character-to-index mapping.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - tokenizer (keras.preprocessing.text.Tokenizer): The fitted Tokenizer object.\n",
        "                - char_to_index (dict): A dictionary mapping each character to its integer index.\n",
        "        \"\"\"\n",
        "        # Instantiate the tokenizer with specific attributes for character-level tokenization\n",
        "        tokenizer = Tokenizer(\n",
        "            filters = '.',      # The period (.) will be removed from the text.\n",
        "            split = '',         # An empty string means no splitting on spaces/other delimiters, tokenizing characters directly.\n",
        "            char_level = True,  # Tokenization will be done at the character level.\n",
        "            lower = False,      # Characters will not be converted to lowercase.\n",
        "            num_words = 45      # Limits the tokenizer to consider only the top 45 most frequent words (characters in this case).\n",
        "        )\n",
        "\n",
        "        # Build the vocabulary from the preprocessed molecule list\n",
        "        tokenizer.fit_on_texts(self.molecule_list)\n",
        "        # Get the character-to-index dictionary\n",
        "        char_to_index = tokenizer.word_index\n",
        "        return tokenizer, char_to_index\n",
        "\n",
        "    def str_to_encode(self):\n",
        "        \"\"\"\n",
        "        Converts the preprocessed SMILES strings into one-hot encoded numerical sequences.\n",
        "        Each character is represented as a binary vector in a sequence.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: A 3D NumPy array of one-hot encoded sequences.\n",
        "                           Shape: (num_molecules, max_length, vocab_size).\n",
        "        \"\"\"\n",
        "        # Convert text sequences to integer sequences based on the vocabulary\n",
        "        sequences = self.tokenizer.texts_to_sequences(self.molecule_list)\n",
        "\n",
        "        # Keras Tokenizer reserves index 0 for padding. To use 0-based indexing for one-hot encoding,\n",
        "        # we subtract 1 from all indices. This assumes no actual character is mapped to 0.\n",
        "        for i in range(len(sequences)):\n",
        "            sequences[i] = np.subtract(sequences[i], np.ones(self.max_length))\n",
        "\n",
        "        # Convert integer sequences to one-hot encoded sequences\n",
        "        # num_classes is the size of the vocabulary (including '0' if it were used)\n",
        "        sequences = to_categorical(sequences, num_classes = len(self.char_to_index) + 1) # +1 for 0-based index compatibility\n",
        "        return sequences\n",
        "\n",
        "    def get_targets(self, sequences):\n",
        "        \"\"\"\n",
        "        Generates target sequences (y) for training a character-level LSTM model.\n",
        "        The target sequence for an input sequence is essentially the input sequence\n",
        "        shifted one position to the left, with the last character replaced by an 'E' token (or padded with 'A').\n",
        "        This means y[i, t, :] is the expected next character after x[i, t-1, :].\n",
        "\n",
        "        Args:\n",
        "            sequences (numpy.ndarray): The one-hot encoded input sequences (X).\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: A 3D NumPy array of target sequences (y).\n",
        "                           Shape: (num_molecules, max_length, vocab_size).\n",
        "        \"\"\"\n",
        "        y = np.zeros(sequences.shape) # Initialize target array with zeros, same shape as input sequences\n",
        "        # For each sequence, the target is the next character in the original sequence.\n",
        "        # Copy all elements except the first, shifting them one position to the left.\n",
        "        # This means y[i, 0:-1, :] gets sequences[i, 1:, :]\n",
        "        for i in range(sequences.shape[0]):\n",
        "          y[i, 0:-1, :] = sequences[i, 1:, :]\n",
        "          # For the very last character position in the target sequence, set the 'A' token (index 0) to 1.\n",
        "          # This ensures that padding is learned as the end target.\n",
        "          if 0 in self.char_to_index.values(): # Check if 'A' is mapped to 0, which it is after subtraction\n",
        "              y[i, -1, 0] = 1 # Set the last element of the sequence in y to represent the padding character 'A'\n",
        "          else:\n",
        "              # If 'A' is not index 0, find its index and set it\n",
        "              padding_char_index = self.char_to_index['A'] # Assuming 'A' is the padding character\n",
        "              y[i, -1, padding_char_index] = 1 # Set the last element of the sequence in y to represent the padding character 'A'\n",
        "        return y\n",
        "\n",
        "    def build_lstm_model(self, num_layers_LSTM = 2):\n",
        "        \"\"\"\n",
        "        Builds and compiles a Sequential Keras LSTM model for character generation.\n",
        "\n",
        "        Args:\n",
        "            num_layers_LSTM (int): The number of LSTM layers to include in the model.\n",
        "                                   Defaults to 2.\n",
        "\n",
        "        Returns:\n",
        "            tensorflow.keras.models.Sequential: The compiled Keras LSTM model.\n",
        "        \"\"\"\n",
        "        # Define the input shape for the LSTM layers: (sequence_length, vocabulary_size)\n",
        "        input_shape = (self.max_length, len(self.char_to_index) + 1) # +1 for 0-based index compatibility\n",
        "        vocab_size = len(self.char_to_index) + 1 # +1 for 0-based index compatibility\n",
        "\n",
        "        # Initialize a sequential model\n",
        "        model = Sequential()\n",
        "\n",
        "        # Add an InputLayer to explicitly define the input shape\n",
        "        model.add(InputLayer(input_shape=input_shape))\n",
        "\n",
        "        # Add specified number of LSTM layers with Dropout\n",
        "        for i in range(num_layers_LSTM):\n",
        "          # LSTM layer with 256 units, returning sequences for stacking, and ReLU activation\n",
        "          model.add(LSTM(256, return_sequences=True, activation='relu'))\n",
        "          # Dropout layer to prevent overfitting\n",
        "          model.add(Dropout(0.2))\n",
        "\n",
        "        # Densely Connected Layer mapping LSTM output to vocabulary size\n",
        "        model.add(Dense(vocab_size))\n",
        "\n",
        "        # Output Layer with Softmax Activation to get probability distribution over vocabulary\n",
        "        model.add(Softmax())\n",
        "\n",
        "        # Compile the model with Adam optimizer and categorical crossentropy loss\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_lstm_model(self, model, sequences, epochs=25, batch_size = 16):\n",
        "        \"\"\"\n",
        "        Trains the provided LSTM model using the generated sequences and targets.\n",
        "\n",
        "        Args:\n",
        "            model (tensorflow.keras.models.Sequential): The compiled Keras LSTM model to be trained.\n",
        "            sequences (numpy.ndarray): The one-hot encoded input sequences (X).\n",
        "            epochs (int): Number of epochs to train the model for. Defaults to 25.\n",
        "            batch_size (int): Number of samples per gradient update. Defaults to 16.\n",
        "\n",
        "        Returns:\n",
        "            tensorflow.keras.callbacks.History: A History object containing training metrics.\n",
        "        \"\"\"\n",
        "        # Create input sequences (X) and target sequences (y)\n",
        "        x = sequences # Dataset\n",
        "        y = self.get_targets(x) # Targets generated from input sequences\n",
        "\n",
        "        # Split the data into training and testing sets for evaluation\n",
        "        train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state = 42)\n",
        "\n",
        "        # Train the LSTM model and return the history object\n",
        "        return model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "    def sample_with_temperature(self, predictions, temperature=0.8):\n",
        "        \"\"\"\n",
        "        Samples an index from a probability distribution, adjusting randomness using a temperature parameter.\n",
        "        Higher temperature (e.g., >1.0) leads to more diverse but less predictable samples.\n",
        "        Lower temperature (e.g., <1.0) makes the model more confident and samples more likely options.\n",
        "\n",
        "        Args:\n",
        "            predictions (numpy.ndarray): An array of predicted probabilities for the next character.\n",
        "            temperature (float): A value controlling the randomness of sampling. Defaults to 0.8.\n",
        "\n",
        "        Returns:\n",
        "            int: The index of the sampled character.\n",
        "        \"\"\"\n",
        "        # Convert predictions to float64 to prevent potential precision issues with log/exp operations\n",
        "        predictions = np.asarray(predictions).astype('float64')\n",
        "\n",
        "        # Apply temperature: divide log probabilities by temperature. This makes probabilities\n",
        "        # for less likely characters higher (more random) if temperature > 1, or lower (less random)\n",
        "        # if temperature < 1.\n",
        "        predictions = np.log(predictions) / temperature\n",
        "        # Exponentiate to convert back to scaled probabilities\n",
        "        exp_predictions = np.exp(predictions)\n",
        "\n",
        "        # Normalize to get a valid probability distribution\n",
        "        predictions = exp_predictions / np.sum(exp_predictions)\n",
        "        # Sample an index using a multinomial distribution based on the scaled probabilities\n",
        "        probabilities = np.random.multinomial(1, predictions, 1)\n",
        "\n",
        "        # Return the index of the sampled character\n",
        "        return np.argmax(probabilities)\n",
        "\n",
        "    def generate_molecule(self, model):\n",
        "        \"\"\"\n",
        "        Generates a new SMILES molecule character by character using the trained LSTM model.\n",
        "        The generation starts with a 'G' token and continues until an 'E' token is predicted\n",
        "        or the `max_length` is reached.\n",
        "\n",
        "        Args:\n",
        "            model (tensorflow.keras.models.Sequential): The trained Keras LSTM model.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated SMILES string.\n",
        "        \"\"\"\n",
        "        start_token = 'G'\n",
        "        generated_molecule = start_token\n",
        "\n",
        "        # Iterate up to max_length - 1 (since 'G' is already present)\n",
        "        for i in range(self.max_length - 1):\n",
        "            # Convert the current generated molecule string to sequence of integers\n",
        "            sequence = self.tokenizer.texts_to_sequences([generated_molecule])[0]\n",
        "\n",
        "            # Pad the sequence to max_length. 'pre' padding means zeros are added at the beginning.\n",
        "            # This ensures the input shape matches what the model expects.\n",
        "            sequence = pad_sequences([sequence], maxlen=self.max_length, padding='pre')\n",
        "\n",
        "            # Adjust indices to be 0-based for one-hot encoding if necessary (tokenizer maps to 1-based by default)\n",
        "            # and convert the integer sequence to one-hot encoded format\n",
        "            sequence_to_encode = np.subtract(sequence, np.ones(sequence.shape, dtype=int))\n",
        "            sequence = to_categorical(sequence_to_encode, num_classes=len(self.char_to_index) + 1)\n",
        "\n",
        "            # Reshape the sequence to match the model's expected input shape (batch_size, max_length, vocab_size)\n",
        "            sequence_new = np.reshape(sequence, (1, self.max_length, len(self.char_to_index) + 1))\n",
        "\n",
        "            # Predict probabilities for the next character at the current position 'i'\n",
        "            # model.predict returns probabilities for each position in the sequence, we take the one for the current token being generated\n",
        "            # The `i` here refers to the actual character being predicted, not the padding index.\n",
        "            predicted_prob = model.predict(sequence_new)[0, i + self.max_length - len(generated_molecule)] # Adjust index based on where the actual sequence starts after 'pre' padding\n",
        "\n",
        "            # Sample the next character index using temperature-controlled sampling\n",
        "            predicted_index_adjusted = self.sample_with_temperature(predicted_prob, temperature=1.0)\n",
        "\n",
        "            # Convert the sampled index back to the original tokenizer index (add 1 if it was 0-based)\n",
        "            if predicted_index_adjusted == 0:\n",
        "                # If index 0 was sampled (which corresponds to 'A' after subtracting 1 in str_to_encode),\n",
        "                # we map it back to the first non-padding character in tokenizer.index_word\n",
        "                predicted_char = self.tokenizer.index_word[1] # Assuming 1 is the actual first char after padding\n",
        "            else:\n",
        "                predicted_char = self.tokenizer.index_word[predicted_index_adjusted + 1] # +1 to get back to 1-based indexing\n",
        "\n",
        "            # Append the predicted character to the generated molecule string\n",
        "            generated_molecule += predicted_char\n",
        "\n",
        "            # Check if the predicted character is the 'E' (end) token, if so, stop generation\n",
        "            if predicted_char == 'E': # Use character directly for comparison\n",
        "                break\n",
        "\n",
        "        return generated_molecule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVt6H126CEDL"
      },
      "source": [
        "# Testing the Complete Pipeline\n",
        "This section loads SMILES data, builds an LSTM model, trains it, and generates new molecules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flGEPQd3gKI2",
        "outputId": "6bd8ce7e-07f7-4c86-c675-c525e6693719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index that represent the char {'A': 1, 'c': 2, 'C': 3, '(': 4, ')': 5, '1': 6, 'n': 7, '2': 8, 'N': 9, 'O': 10, '3': 11, 'G': 12, 'E': 13, '-': 14, '=': 15, '4': 16, '#': 17, '[': 18, 'H': 19, ']': 20, 'L': 21, 's': 22, 'F': 23}\n",
            "Sequences do str_to_encode [[12, 3, 10, 2, 6, 2, 2, 2, 2, 2, 6, 14, 2, 6, 2, 2, 2, 7, 8, 7, 2, 4, 9, 5, 7, 2, 6, 8, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 9, 3, 4, 15, 10, 5, 2, 6, 2, 2, 4, 14, 2, 8, 2, 2, 2, 2, 2, 8, 5, 22, 2, 6, 9, 2, 6, 2, 2, 2, 7, 2, 6, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 9, 3, 3, 7, 6, 2, 4, 3, 8, 3, 3, 9, 3, 3, 8, 5, 7, 2, 8, 2, 2, 4, 3, 4, 9, 5, 15, 10, 5, 2, 2, 2, 8, 6, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 9, 3, 4, 15, 10, 5, 2, 6, 2, 2, 2, 8, 2, 4, 2, 6, 5, 7, 2, 4, 3, 6, 3, 3, 3, 4, 10, 5, 3, 3, 6, 5, 7, 8, 3, 3, 10, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 3, 2, 6, 2, 2, 2, 4, 14, 2, 8, 7, 2, 11, 2, 2, 4, 3, 4, 9, 5, 15, 10, 5, 2, 2, 2, 11, 7, 8, 3, 3, 9, 5, 2, 2, 6, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 3, 10, 3, 3, 4, 3, 5, 9, 2, 6, 2, 2, 7, 2, 4, 14, 7, 8, 2, 7, 2, 11, 2, 2, 2, 4, 3, 17, 9, 5, 2, 2, 11, 8, 5, 7, 6, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 9, 3, 4, 15, 10, 5, 2, 6, 2, 2, 2, 8, 2, 4, 2, 6, 5, 7, 2, 4, 3, 6, 3, 3, 3, 4, 10, 5, 3, 3, 6, 5, 7, 8, 3, 3, 3, 10, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 3, 3, 4, 3, 5, 3, 4, 3, 5, 9, 3, 4, 15, 10, 5, 2, 6, 2, 18, 7, 19, 20, 2, 8, 7, 2, 2, 4, 3, 11, 3, 3, 11, 5, 7, 2, 6, 8, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 3, 3, 4, 2, 6, 2, 2, 2, 2, 2, 6, 5, 9, 4, 3, 5, 2, 6, 2, 2, 7, 2, 4, 14, 7, 8, 2, 7, 2, 11, 2, 2, 7, 2, 2, 11, 8, 5, 7, 6, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 3, 10, 2, 6, 2, 2, 4, 9, 2, 8, 7, 2, 2, 11, 2, 2, 2, 4, 14, 2, 16, 2, 2, 2, 2, 2, 16, 5, 7, 11, 7, 8, 5, 2, 2, 4, 10, 3, 5, 2, 6, 10, 3, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 3, 10, 3, 2, 6, 2, 2, 2, 2, 2, 6, 14, 2, 6, 2, 2, 2, 2, 8, 7, 2, 4, 9, 2, 11, 2, 2, 2, 16, 2, 4, 2, 11, 5, 3, 3, 9, 3, 3, 16, 5, 7, 7, 6, 8, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 9, 3, 4, 15, 10, 5, 2, 6, 2, 2, 4, 14, 2, 8, 2, 2, 7, 2, 4, 9, 5, 7, 8, 5, 18, 7, 19, 20, 2, 6, 14, 2, 6, 2, 2, 4, 21, 5, 2, 2, 2, 6, 21, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 3, 10, 2, 6, 2, 2, 2, 4, 14, 2, 8, 7, 2, 11, 18, 7, 19, 20, 2, 2, 4, 21, 5, 2, 4, 9, 3, 3, 16, 3, 3, 9, 3, 16, 5, 2, 14, 11, 7, 8, 5, 2, 2, 6, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 3, 9, 4, 3, 5, 3, 6, 3, 3, 3, 4, 7, 8, 2, 2, 4, 3, 4, 9, 5, 15, 10, 5, 2, 4, 9, 2, 11, 2, 2, 2, 2, 2, 11, 5, 7, 8, 5, 3, 4, 3, 17, 9, 5, 3, 6, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [12, 9, 17, 3, 3, 6, 3, 3, 8, 4, 3, 3, 3, 6, 7, 6, 2, 2, 4, 3, 4, 9, 5, 15, 10, 5, 2, 4, 9, 2, 11, 2, 2, 7, 2, 4, 23, 5, 2, 11, 5, 7, 6, 5, 3, 3, 8, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
            "Input shape:  (100, 23)\n",
            "vocab_size:  23\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_6 (LSTM)               (None, 100, 256)          286720    \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 100, 256)          0         \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 100, 256)          525312    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 100, 256)          0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 100, 23)           5911      \n",
            "                                                                 \n",
            " softmax_3 (Softmax)         (None, 100, 23)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 817943 (3.12 MB)\n",
            "Trainable params: 817943 (3.12 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/25\n",
            "1/1 [==============================] - 3s 3s/step - loss: 3.1420 - accuracy: 0.0033\n",
            "Epoch 2/25\n",
            "1/1 [==============================] - 0s 373ms/step - loss: 3.0876 - accuracy: 0.6050\n",
            "Epoch 3/25\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 3.0316 - accuracy: 0.5958\n",
            "Epoch 4/25\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 2.9583 - accuracy: 0.5950\n",
            "Epoch 5/25\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 2.8443 - accuracy: 0.5950\n",
            "Epoch 6/25\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 2.6199 - accuracy: 0.5950\n",
            "Epoch 7/25\n",
            "1/1 [==============================] - 0s 389ms/step - loss: 1.9330 - accuracy: 0.5950\n",
            "Epoch 8/25\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 4.6631 - accuracy: 0.5933\n",
            "Epoch 9/25\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 1.4528 - accuracy: 0.5950\n",
            "Epoch 10/25\n",
            "1/1 [==============================] - 0s 388ms/step - loss: 2.1051 - accuracy: 0.5950\n",
            "Epoch 11/25\n",
            "1/1 [==============================] - 0s 373ms/step - loss: 2.3920 - accuracy: 0.5950\n",
            "Epoch 12/25\n",
            "1/1 [==============================] - 0s 373ms/step - loss: 2.5406 - accuracy: 0.5950\n",
            "Epoch 13/25\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 2.6311 - accuracy: 0.5950\n",
            "Epoch 14/25\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 2.6920 - accuracy: 0.5950\n",
            "Epoch 15/25\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 2.7383 - accuracy: 0.5950\n",
            "Epoch 16/25\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 2.7703 - accuracy: 0.5950\n",
            "Epoch 17/25\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 2.7944 - accuracy: 0.5950\n",
            "Epoch 18/25\n",
            "1/1 [==============================] - 1s 610ms/step - loss: 2.8168 - accuracy: 0.5950\n",
            "Epoch 19/25\n",
            "1/1 [==============================] - 1s 589ms/step - loss: 2.8329 - accuracy: 0.5950\n",
            "Epoch 20/25\n",
            "1/1 [==============================] - 1s 643ms/step - loss: 2.8454 - accuracy: 0.5958\n",
            "Epoch 21/25\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 2.8573 - accuracy: 0.5958\n",
            "Epoch 22/25\n",
            "1/1 [==============================] - 1s 644ms/step - loss: 2.8657 - accuracy: 0.5975\n",
            "Epoch 23/25\n",
            "1/1 [==============================] - 1s 586ms/step - loss: 2.8717 - accuracy: 0.5975\n",
            "Epoch 24/25\n",
            "1/1 [==============================] - 1s 600ms/step - loss: 2.8798 - accuracy: 0.5975\n",
            "Epoch 25/25\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 2.8858 - accuracy: 0.5958\n",
            "None\n",
            "Sequence before pad:  [12]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0 12]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Predicted prob:  [0.04608145 0.04538468 0.04536526 0.04334805 0.04369111 0.04333699\n",
            " 0.0432665  0.04318184 0.04312772 0.04310887 0.04310451 0.04314636\n",
            " 0.04319907 0.04298951 0.04306208 0.04307116 0.04311935 0.0429931\n",
            " 0.04301265 0.04316434 0.04303327 0.04307336 0.04313881]\n",
            "Predicted index:  20\n",
            "Predicted char:  ]\n",
            "Sequence before pad:  [12, 20]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0 12 20]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "Predicted prob:  [0.04721327 0.04545511 0.04546677 0.04349599 0.04363602 0.04330262\n",
            " 0.0432495  0.0430811  0.04302735 0.04303113 0.04306377 0.04315739\n",
            " 0.04328809 0.04274812 0.04293782 0.04296375 0.04311541 0.04276436\n",
            " 0.04282963 0.04319222 0.04286549 0.0429799  0.04313517]\n",
            "Predicted index:  16\n",
            "Predicted char:  4\n",
            "Sequence before pad:  [12, 20, 16]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0 12 20 16]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "Predicted prob:  [0.04852064 0.04553181 0.04557019 0.04367859 0.04357016 0.04325719\n",
            " 0.04321923 0.0429683  0.04289597 0.04294358 0.04303468 0.04316333\n",
            " 0.04339636 0.04248447 0.04278688 0.04284379 0.04311454 0.04249699\n",
            " 0.04262734 0.04321205 0.04266713 0.04287774 0.04313898]\n",
            "Predicted index:  1\n",
            "Predicted char:  A\n",
            "Sequence before pad:  [12, 20, 16, 1]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  12 20 16  1]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "Predicted prob:  [0.04991478 0.04561192 0.04566715 0.04388232 0.04349733 0.04321027\n",
            " 0.04317567 0.04285271 0.04274676 0.04284812 0.04301611 0.04316456\n",
            " 0.04351043 0.04221521 0.04262541 0.04271766 0.04310907 0.04221237\n",
            " 0.04242634 0.04321633 0.04245476 0.04277453 0.04315034]\n",
            "Predicted index:  16\n",
            "Predicted char:  4\n",
            "Sequence before pad:  [12, 20, 16, 1, 16]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12\n",
            "  20 16  1 16]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "Predicted prob:  [0.05133624 0.0456915  0.04575532 0.04409937 0.04342048 0.04317329\n",
            " 0.04312679 0.04273496 0.04259508 0.04274359 0.04300643 0.0431635\n",
            " 0.04362334 0.04195263 0.0424657  0.04259012 0.0430856  0.04192727\n",
            " 0.04223207 0.04320334 0.04223721 0.04266889 0.04316733]\n",
            "Predicted index:  19\n",
            "Predicted char:  H\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 20\n",
            "  16  1 16 19]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "Predicted prob:  [0.05273685 0.04576726 0.04583716 0.04432106 0.04334467 0.04314787\n",
            " 0.04307683 0.04261675 0.04244835 0.04263206 0.04300437 0.04315878\n",
            " 0.0437329  0.04170278 0.04231419 0.04246622 0.04304384 0.04165277\n",
            " 0.04204967 0.04317107 0.042022   0.04256584 0.04318657]\n",
            "Predicted index:  6\n",
            "Predicted char:  1\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 20 16\n",
            "   1 16 19  6]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "Predicted prob:  [0.05408119 0.04583936 0.04591174 0.04454166 0.04327225 0.04313168\n",
            " 0.04302691 0.04250199 0.04231071 0.04251769 0.04300882 0.0431507\n",
            " 0.04383625 0.04146905 0.04217448 0.04234924 0.04298695 0.04139493\n",
            " 0.04188245 0.0431213  0.04181567 0.04246848 0.04320655]\n",
            "Predicted index:  20\n",
            "Predicted char:  ]\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 20 16  1\n",
            "  16 19  6 20]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "Predicted prob:  [0.05534623 0.04590796 0.04597913 0.04475701 0.04320438 0.04312236\n",
            " 0.04297802 0.04239427 0.04218486 0.04240317 0.04301848 0.04314017\n",
            " 0.04393112 0.04125234 0.04204727 0.04224033 0.04291894 0.04115763\n",
            " 0.04173144 0.04305685 0.04162313 0.04237922 0.04322575]\n",
            "Predicted index:  22\n",
            "Predicted char:  s\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20, 22]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 20 16  1 16\n",
            "  19  6 20 22]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "Predicted prob:  [0.05651785 0.04597339 0.0460387  0.04496332 0.04314113 0.04311873\n",
            " 0.04293158 0.04229503 0.04207171 0.04229125 0.04303215 0.04312859\n",
            " 0.04401695 0.0410537  0.04193391 0.04214059 0.04284298 0.04094232\n",
            " 0.04159647 0.04298164 0.04144618 0.04229862 0.0432432 ]\n",
            "Predicted index:  5\n",
            "Predicted char:  )\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20, 22, 5]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 20 16  1 16 19\n",
            "   6 20 22  5]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "Predicted prob:  [0.05758893 0.04603576 0.04609119 0.04515816 0.04308325 0.04311951\n",
            " 0.04288818 0.0422047  0.04197175 0.04218416 0.04304838 0.04311706\n",
            " 0.04409346 0.04087289 0.04183444 0.04205013 0.04276227 0.04074889\n",
            " 0.04147666 0.04289901 0.0412855  0.04222711 0.04325857]\n",
            "Predicted index:  14\n",
            "Predicted char:  -\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20, 22, 5, 14]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 20 16  1 16 19  6\n",
            "  20 22  5 14]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "Predicted prob:  [0.05855743 0.04609571 0.04613831 0.04534004 0.04303168 0.04312424\n",
            " 0.04284738 0.04212382 0.04188477 0.04208346 0.04306576 0.04310617\n",
            " 0.04416031 0.04070865 0.04174737 0.04196807 0.04267933 0.04057646\n",
            " 0.04137016 0.04281216 0.04114155 0.04216526 0.04327178]\n",
            "Predicted index:  0\n",
            "Predicted char:  A\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20, 22, 5, 14, 1]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 20 16  1 16 19  6 20\n",
            "  22  5 14  1]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "Predicted prob:  [0.05942524 0.04615149 0.04618072 0.04550763 0.04298712 0.04313179\n",
            " 0.04280894 0.04205224 0.0418108  0.04198982 0.04308401 0.04309708\n",
            " 0.04421724 0.04056069 0.04167213 0.0418956  0.04259547 0.04042425\n",
            " 0.04127646 0.04272233 0.04101269 0.04211287 0.04328323]\n",
            "Predicted index:  5\n",
            "Predicted char:  )\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20, 22, 5, 14, 1, 5]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 20 16  1 16 19  6 20 22\n",
            "   5 14  1  5]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "Predicted prob:  [0.06019681 0.04620254 0.0462185  0.04566077 0.04294956 0.04314099\n",
            " 0.04277304 0.04198954 0.0417486  0.04190391 0.0431025  0.04309012\n",
            " 0.04426502 0.04042817 0.04160774 0.04183277 0.0425126  0.04029129\n",
            " 0.04119465 0.04263126 0.04089782 0.04206888 0.04329288]\n",
            "Predicted index:  8\n",
            "Predicted char:  2\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20, 22, 5, 14, 1, 5, 8]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 20 16  1 16 19  6 20 22  5\n",
            "  14  1  5  8]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "Predicted prob:  [0.06087859 0.04624793 0.04625267 0.04580028 0.04291959 0.04315007\n",
            " 0.04274143 0.0419355  0.04169573 0.04182718 0.04312098 0.04308477\n",
            " 0.04430655 0.04030911 0.0415523  0.04177924 0.04243373 0.04017493\n",
            " 0.04112206 0.04254209 0.04079559 0.04203008 0.04329969]\n",
            "Predicted index:  18\n",
            "Predicted char:  [\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20, 22, 5, 14, 1, 5, 8, 18]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0 12 20 16  1 16 19  6 20 22  5 14\n",
            "   1  5  8 18]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "Predicted prob:  [0.06147744 0.04628823 0.0462833  0.04592687 0.04289592 0.04315887\n",
            " 0.04271377 0.04188916 0.04165096 0.04175884 0.04313875 0.04308057\n",
            " 0.04434239 0.04020265 0.0415048  0.04173366 0.04236012 0.04007348\n",
            " 0.04105788 0.04245671 0.0407053  0.04199605 0.04330424]\n",
            "Predicted index:  1\n",
            "Predicted char:  A\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20, 22, 5, 14, 1, 5, 8, 18, 1]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0 12 20 16  1 16 19  6 20 22  5 14  1\n",
            "   5  8 18  1]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "Predicted prob:  [0.06200078 0.04632412 0.04631053 0.0460412  0.04287732 0.04316738\n",
            " 0.04268958 0.04184956 0.04161323 0.04169804 0.04315533 0.04307719\n",
            " 0.04437307 0.04010798 0.04146433 0.04169491 0.04229246 0.03998547\n",
            " 0.04100153 0.04237634 0.04062617 0.04196649 0.04330704]\n",
            "Predicted index:  20\n",
            "Predicted char:  ]\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20, 22, 5, 14, 1, 5, 8, 18, 1, 20]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0 12 20 16  1 16 19  6 20 22  5 14  1  5\n",
            "   8 18  1 20]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "Predicted prob:  [0.06245612 0.04635608 0.04633452 0.04614399 0.0428628  0.04317552\n",
            " 0.04266844 0.0418158  0.04158151 0.04164398 0.04317046 0.04307438\n",
            " 0.04439915 0.04002411 0.04142998 0.04166196 0.042231   0.03990943\n",
            " 0.04095229 0.04230171 0.0405572  0.04194102 0.04330847]\n",
            "Predicted index:  9\n",
            "Predicted char:  N\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20, 22, 5, 14, 1, 5, 8, 18, 1, 20, 9]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0 12 20 16  1 16 19  6 20 22  5 14  1  5  8\n",
            "  18  1 20  9]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "Predicted prob:  [0.06285087 0.04638454 0.04635562 0.046236   0.04285145 0.04318337\n",
            " 0.04265003 0.04178714 0.04155501 0.04159589 0.04318394 0.04307197\n",
            " 0.0444213  0.03995    0.04140095 0.04163402 0.04217576 0.03984386\n",
            " 0.04090953 0.04223331 0.04049738 0.04191922 0.04330884]\n",
            "Predicted index:  4\n",
            "Predicted char:  (\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20, 22, 5, 14, 1, 5, 8, 18, 1, 20, 9, 4]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0 12 20 16  1 16 19  6 20 22  5 14  1  5  8 18\n",
            "   1 20  9  4]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "Predicted prob:  [0.06319207 0.04640985 0.04637415 0.04631797 0.04284251 0.04319093\n",
            " 0.04263401 0.04176296 0.04153302 0.04155309 0.04319569 0.04306988\n",
            " 0.04444011 0.03988457 0.04137646 0.04161038 0.04212653 0.03978732\n",
            " 0.04087256 0.0421713  0.04044568 0.04190061 0.04330829]\n",
            "Predicted index:  0\n",
            "Predicted char:  A\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20, 22, 5, 14, 1, 5, 8, 18, 1, 20, 9, 4, 1]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0 12 20 16  1 16 19  6 20 22  5 14  1  5  8 18  1\n",
            "  20  9  4  1]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "Predicted prob:  [0.06348623 0.04643235 0.04639034 0.04639077 0.04283553 0.04319815\n",
            " 0.04262009 0.04174256 0.04151479 0.041515   0.04320587 0.04306798\n",
            " 0.04445603 0.03982698 0.04135587 0.04159041 0.04208297 0.03973872\n",
            " 0.04084069 0.04211553 0.04040111 0.04188484 0.04330709]\n",
            "Predicted index:  11\n",
            "Predicted char:  3\n",
            "Sequence before pad:  [12, 20, 16, 1, 16, 19, 6, 20, 22, 5, 14, 1, 5, 8, 18, 1, 20, 9, 4, 1, 11]\n",
            "Sequence after pad:  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0 12 20 16  1 16 19  6 20 22  5 14  1  5  8 18  1 20\n",
            "   9  4  1 11]]\n",
            "Sequence after categorical:  [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "Sequence after reshape:  (1, 100, 23)\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "Predicted prob:  [0.06373932 0.04645234 0.04640442 0.04645522 0.04283012 0.04320499\n",
            " 0.04260797 0.0417254  0.0414997  0.04148114 0.04321463 0.04306623\n",
            " 0.04446949 0.03977644 0.04133859 0.04157355 0.04204466 0.03969706\n",
            " 0.04081329 0.04206572 0.04036279 0.04187157 0.04330543]\n",
            "Predicted index:  13\n",
            "Predicted char:  E\n",
            "G]4A4H1]s)-A)2[A]N(A3E\n"
          ]
        }
      ],
      "source": [
        "generating = SmilesToOneHotEncoding(\"/data_50_len100.txt\",15,20,100)\n",
        "\n",
        "# Load file + Translation + Padding\n",
        "list_of_molecules = generating.preprocess_data()\n",
        "#print(list_of_molecules)\n",
        "\n",
        "# Make Vocabulary\n",
        "tokenizer, char_to_index = generating.make_vocabulary()\n",
        "print(\"Index that represent the char\",char_to_index)\n",
        "\n",
        "# String to Encode\n",
        "sequence = generating.str_to_encode()\n",
        "#for i in range(len(sequence)):\n",
        "#  print(sequence[i].shape)\n",
        "\n",
        "# Build LSTM model\n",
        "model = generating.build_lstm_model(num_layers_LSTM = 2)\n",
        "print(model.summary())\n",
        "\n",
        "# Train LSTM model\n",
        "training = generating.train_lstm_model(model, sequence, 25, 16)\n",
        "print(training)\n",
        "\n",
        "# Generate Molecule (1)\n",
        "generate_molecule = generating.generate_molecule(model)\n",
        "print(generate_molecule)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
